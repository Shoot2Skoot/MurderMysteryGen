# Mystery.AI - Testing Strategy (MVP)

This document outlines the testing strategy for the Minimum Viable Product (MVP) of the Mystery.AI system. The strategy focuses on ensuring the core functionality is working as expected, the generated data is sound, and the system is usable for its primary user (the developer/author).

## 1. Testing Goals (MVP - Foundational System)

-   **Verify Core Logic:** Ensure the MMO framework generation, killer selection, MMO modification, and initial evidence generation logic function correctly according to the PRD for the foundational system.
-   **Validate Data Output:** Confirm that the output JSON is well-structured, complete, and adheres to the defined schema in `docs/data-models.md`.
-   **Ensure Agent Orchestration:** Test the successful sequential execution of agents and data handoff within the pipeline.
-   **Usability for Developer:** Confirm the CLI tool for the foundational system is easy to run and provides sufficient feedback (logs, output) for the developer.
-   **Identify Major Coherence Issues:** While deep narrative coherence is post-MVP, initial manual review aims to catch glaring logical inconsistencies in the generated mystery framework.

## 2. Testing Levels & Types (MVP - Foundational System)

### 2.1. Manual Testing / Review (Primary Focus for Content Quality - Foundational System)

-   **Scope:** Review of the final `CaseContext` JSON output generated by the foundational system.
-   **Process:**
    1.  Run the main generation script (`run_mystery_generation.py`) with various test themes.
    2.  Manually inspect the output JSON file.
    3.  Check for:
        -   Completeness: Are all expected sections and data points present (victim, 2-3 suspects, killer designated, MMOs for all, modified MMOs for non-killers, evidence for all)?
        -   MMO Coherence:
            -   Do the original MMOs for each suspect make logical sense within the given theme and for the victim?
            -   Is the designated killer's MMO plausible?
            -   For non-killers, is one MMO element clearly weakened/invalidated in a way that makes them less likely, but still a red herring?
        -   Evidence Relevance:
            -   Does evidence for the killer support their true MMO?
            -   Does red herring evidence for non-killers plausibly point to their original (now weakened) MMO elements?
        -   Overall Plausibility: Does the core mystery structure feel like a viable starting point for a story?
-   **Tools:** Text editor, JSON viewer/formatter.
-   **Success Criteria:** Aligns with "MMO Coherence Rating" and "Evidence Relevance Rating" KPIs in the PRD (e.g., >70% of outputs rated as sufficiently coherent by the primary user).

### 2.2. Automated Unit Tests (Python `unittest` or `pytest` - Foundational System)

-   **Scope:** Focus on small, isolated pieces of code (utility functions, individual methods within foundational agents if they contain complex, non-LLM-dependent logic).
-   **Targets:**
    -   `src/mystery_ai/core/utils.py` (if any utilities are created).
    -   `src/mystery_ai/core/data_models.py`: Test Pydantic model validation for expected valid/invalid data (though Pydantic handles much of this, custom validators could be tested).
    -   Helper functions within agent modules that *do not* directly involve LLM calls (e.g., a function that formats data before sending to an LLM, or a function that selects a random element from a list for MMO modification strategy).
-   **Tools:** `unittest` (Python built-in) or `pytest` (if preferred for more features).
-   **Goal:** Ensure basic code correctness and prevent regressions in utility/helper logic. Not intended to test LLM outputs directly at this level.

### 2.3. Automated Integration Tests (Python `unittest` or `pytest` - Foundational System)

-   **Scope:** Test the interaction and data flow between a few key foundational agents or small segments of the foundational pipeline.
-   **Targets:**
    -   **Orchestrator to First Agent:** Test that the main orchestrator script can correctly instantiate and run the `CaseInitializationAgent` and receive back a valid `CaseContext` (with `VictimProfile`).
    -   **Data Model Integrity through Pipeline:** Mock agent LLM calls to return predefined structured data. Test that this data is correctly passed through a sequence of 2-3 agents and that the `CaseContext` is updated as expected.
    -   **JSON Output Validation:**
        -   Run the full pipeline with mocked LLM responses (to ensure predictable output).
        -   Programmatically load the output JSON file.
        -   Validate its structure against the JSON schema derived from `CaseContext.model_json_schema()`.
-   **Tools:** `unittest` or `pytest`, mocking libraries (e.g., `unittest.mock`).
-   **Goal:** Verify that agents can be called in sequence, data is passed correctly, and the final output adheres to the schema.

### 2.4. End-to-End (E2E) "Smoke" Tests (Foundational System)

-   **Scope:** Run the entire foundational generation pipeline (e.g., `run_mystery_generation.py`) with a few predefined themes.
-   **Process:**
    1.  Execute the main script.
    2.  Check that the script completes without unhandled exceptions.
    3.  Verify that an output JSON file is created.
    4.  Perform a quick, high-level check of the JSON file for presence of major sections (victim, suspects, evidence).
-   **Tools:** Shell scripts, Python script to automate execution and basic checks.
-   **Goal:** Ensure the basic pipeline is runnable and produces an output file. This is a "smoke test" to catch major breakages, not for detailed content validation (which is manual).

## 3. What Not Being Tested (for MVP - Applies to Both Systems where relevant)

-   **Deep Narrative Coherence/Quality via Automation:** Automated testing of the creative quality, engagement, or deep logical consistency of the LLM-generated narratives is out of scope for MVP. This relies on manual review.
-   **LLM Performance/Accuracy Directly:** We are not building a test suite to evaluate the LLMs themselves, but rather how our agent system uses them.
-   **UI/UX Testing:** No user interface beyond the CLI for the developer.
-   **Load/Stress Testing:** The system is a single-user CLI tool for MVP.
-   **Security Vulnerability Testing (beyond API key handling):** Not a focus for local CLI tool with no external network exposure beyond OpenAI API.

## 4. Test Environment & Data (Applies to Both Systems)

-   **Environment:** Local developer machine with Python environment set up as per `requirements.txt`. OpenAI API key must be configured.
-   **Test Data:**
    -   Input themes for testing will be simple strings (e.g., "Cyberpunk", "Haunted Mansion", "Pirate Ship").
    -   For unit/integration tests requiring predictable LLM-like output, mocked responses or fixture data (small, valid JSON snippets matching Pydantic models) will be used.

## 5. Defect Tracking & Reporting (Applies to Both Systems)

-   For this internal MVP, defects found during manual review or by automated tests will be tracked informally (e.g., a simple list, comments in code, or a basic issue tracker if the user prefers).
-   The primary goal is to identify issues for the developer/author to iterate on the agent prompts and logic.

This testing strategy aims to provide sufficient confidence in the MVP's core functionality while remaining practical for a rapid development cycle.

## 6. Testing Strategy for Branching Evidence System MVP

This section outlines the testing strategy specifically for the Branching Evidence System MVP, which builds upon a foundational mystery context.

### 6.1. Testing Goals (Branching Evidence MVP)

-   **Verify Core Logic:** Ensure the MVP logic of `NarrativeRefinementAgent`, `TimelineOrchestratorAgent`, `InformationBlueprintAgent`, `ClueWeavingAgent`, `EvidenceDistributionAgent`, and `MasterCoherenceAgent` functions as per their epic definitions.
-   **Validate Data Output:** Confirm that the output `BranchingCaseContext` JSON is well-structured, complete, and adheres to the schema defined in `docs/data-models.md` (Branching Evidence section).
-   **Ensure Agent Orchestration:** Test the successful sequential execution of the branching evidence agents and data handoff within its specific pipeline (e.g., driven by `run_full_branching_pipeline.py`).
-   **Usability of CLI Tools:** Confirm the new CLI tools for running individual branching agents/epics and the full branching pipeline are usable and provide sufficient feedback.
-   **Validate Basic Solvability by Elimination:** Use the `MasterCoherenceAgent` (MVP) and manual review to check if the generated branching evidence structure supports basic elimination of non-killers and a plausible (non-obvious) path for the killer, based on `SUPPORTED` nuggets.

### 6.2. Testing Levels & Types (Branching Evidence MVP)

#### 6.2.1. Manual Testing / Review

-   **Scope:** Review of the final `BranchingCaseContext` JSON output from the branching evidence pipeline.
-   **Process:**
    1.  Run the main branching evidence generation script (e.g., `run_full_branching_pipeline.py`) with various test foundational `BranchingCaseContext` inputs and map files.
    2.  Manually inspect the output JSON file.
    3.  Check for:
        -   **Completeness:** Are all expected sections present (timeline, character movements, locations, core murder action details, information nuggets, information fragments, branching evidence items)?
        -   **Timeline Coherence:** Does the generated timeline (`num_stages`, `critical_action_window_stages`, events, character movements) make sense with the narrative and map?
        -   **Alibi and Killer Path Support:** Do `InformationNugget` and `InformationFragment` definitions plausibly support non-killer alibis and the killer's path (as per MVP blueprinting logic)?
        -   **Evidence Content:** Do `BranchingEvidenceItem` descriptions contain the `raw_data_from_evidence` for their constituent fragments? Is the `category` and `discovery_details` (MVP level) appropriate?
        -   **Basic Solvability:** Can non-killers be eliminated based on their alibi nuggets being `SUPPORTED`? Is the killer's guilt non-obvious but deducible through elimination?
-   **Tools:** Text editor, JSON viewer/formatter.
-   **Success Criteria:** Alignment with `MasterCoherenceAgent` MVP validation; manual assessment confirms a workable structure for an elimination puzzle.

#### 6.2.2. Automated Unit Tests

-   **Scope:** Small, isolated pieces of code within the Branching Evidence System.
-   **Targets:**
    -   `src/mystery_ai/core/data_models_branching.py`: Pydantic model validation for `BranchingCaseContext` and its components.
    -   Helper functions within the new branching agents (non-LLM dependent logic, e.g., logic for determining stage transitions, simple data transformations).
    -   Core logic within `MasterCoherenceAgent` (MVP version) for evaluating nugget statuses (e.g., assuming fragments are found to make nuggets `SUPPORTED`) and checking alibi conditions based on these statuses.
-   **Tools:** `unittest` or `pytest`.

#### 6.2.3. Automated Integration Tests

-   **Scope:** Interaction and data flow between key branching agents or segments of the branching pipeline.
-   **Targets:**
    -   **Orchestrator to First Branching Agent:** Test that the main branching pipeline script can correctly invoke the `NarrativeRefinementAgent` with an initial `BranchingCaseContext` and map data, receiving back an updated context.
    -   **Data Model Integrity through Branching Pipeline:** Mock LLM calls for branching agents to return predefined structured data/updates. Test that the `BranchingCaseContext` is correctly and progressively updated as it passes through a sequence of key branching agents (e.g., `NarrativeRefinementAgent` -> `TimelineOrchestratorAgent` -> `InformationBlueprintAgent`).
    -   **JSON Output Validation (Branching):** Run the full branching pipeline with mocked LLM responses. Programmatically load the output `BranchingCaseContext` JSON and validate its structure against the schema from `BranchingCaseContext.model_json_schema()`.
-   **Tools:** `unittest` or `pytest`, mocking libraries.

#### 6.2.4. End-to-End (E2E) "Smoke" Tests

-   **Scope:** Run the entire Branching Evidence System generation pipeline (e.g., `run_full_branching_pipeline.py`) with sample input files.
-   **Process:**
    1.  Execute the main script.
    2.  Check that the script completes without unhandled exceptions.
    3.  Verify that an output `BranchingCaseContext` JSON file is created.
    4.  Perform a quick, high-level check of the JSON for presence of major branching-specific sections (timeline, nuggets, branching_evidence_items).
-   **Tools:** Shell scripts, Python script to automate execution.

This testing strategy, combining foundational and branching evidence specific approaches, aims to provide sufficient confidence in the overall Mystery.AI system's capabilities as it evolves. 