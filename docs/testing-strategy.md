# Mystery.AI - Testing Strategy (MVP)

This document outlines the testing strategy for the Minimum Viable Product (MVP) of the Mystery.AI system. The strategy focuses on ensuring the core functionality is working as expected, the generated data is sound, and the system is usable for its primary user (the developer/author).

## 1. Testing Goals (MVP)

-   **Verify Core Logic:** Ensure the MMO framework generation, killer selection, MMO modification, and initial evidence generation logic function correctly according to the PRD.
-   **Validate Data Output:** Confirm that the output JSON is well-structured, complete, and adheres to the defined schema in `docs/data-models.md`.
-   **Ensure Agent Orchestration:** Test the successful sequential execution of agents and data handoff within the pipeline.
-   **Usability for Developer:** Confirm the CLI tool is easy to run and provides sufficient feedback (logs, output) for the developer.
-   **Identify Major Coherence Issues:** While deep narrative coherence is post-MVP, initial manual review aims to catch glaring logical inconsistencies in the generated mystery framework.

## 2. Testing Levels & Types (MVP)

### 2.1. Manual Testing / Review (Primary Focus for Content Quality)

-   **Scope:** Review of the final JSON output generated by the system.
-   **Process:**
    1.  Run the main generation script (`run_mystery_generation.py`) with various test themes.
    2.  Manually inspect the output JSON file.
    3.  Check for:
        -   Completeness: Are all expected sections and data points present (victim, 2-3 suspects, killer designated, MMOs for all, modified MMOs for non-killers, evidence for all)?
        -   MMO Coherence:
            -   Do the original MMOs for each suspect make logical sense within the given theme and for the victim?
            -   Is the designated killer's MMO plausible?
            -   For non-killers, is one MMO element clearly weakened/invalidated in a way that makes them less likely, but still a red herring?
        -   Evidence Relevance:
            -   Does evidence for the killer support their true MMO?
            -   Does red herring evidence for non-killers plausibly point to their original (now weakened) MMO elements?
        -   Overall Plausibility: Does the core mystery structure feel like a viable starting point for a story?
-   **Tools:** Text editor, JSON viewer/formatter.
-   **Success Criteria:** Aligns with "MMO Coherence Rating" and "Evidence Relevance Rating" KPIs in the PRD (e.g., >70% of outputs rated as sufficiently coherent by the primary user).

### 2.2. Automated Unit Tests (Python `unittest` or `pytest`)

-   **Scope:** Focus on small, isolated pieces of code (utility functions, individual methods within agents if they contain complex, non-LLM-dependent logic).
-   **Targets:**
    -   `src/mystery_ai/core/utils.py` (if any utilities are created).
    -   `src/mystery_ai/core/data_models.py`: Test Pydantic model validation for expected valid/invalid data (though Pydantic handles much of this, custom validators could be tested).
    -   Helper functions within agent modules that *do not* directly involve LLM calls (e.g., a function that formats data before sending to an LLM, or a function that selects a random element from a list for MMO modification strategy).
-   **Tools:** `unittest` (Python built-in) or `pytest` (if preferred for more features).
-   **Goal:** Ensure basic code correctness and prevent regressions in utility/helper logic. Not intended to test LLM outputs directly at this level.

### 2.3. Automated Integration Tests (Python `unittest` or `pytest`)

-   **Scope:** Test the interaction and data flow between a few key agents or small segments of the pipeline. This is more about checking the "plumbing" than the quality of LLM-generated content.
-   **Targets:**
    -   **Orchestrator to First Agent:** Test that the main orchestrator script can correctly instantiate and run the `CaseInitializationAgent` and receive back a valid `CaseContext` (with `VictimProfile`).
    -   **Data Model Integrity through Pipeline:** Mock agent LLM calls to return predefined structured data. Test that this data is correctly passed through a sequence of 2-3 agents and that the `CaseContext` is updated as expected.
    -   **JSON Output Validation:**
        -   Run the full pipeline with mocked LLM responses (to ensure predictable output).
        -   Programmatically load the output JSON file.
        -   Validate its structure against the JSON schema derived from `CaseContext.model_json_schema()`.
-   **Tools:** `unittest` or `pytest`, mocking libraries (e.g., `unittest.mock`).
-   **Goal:** Verify that agents can be called in sequence, data is passed correctly, and the final output adheres to the schema.

### 2.4. End-to-End (E2E) "Smoke" Tests

-   **Scope:** Run the entire generation pipeline (`run_mystery_generation.py`) with a few predefined themes.
-   **Process:**
    1.  Execute the main script.
    2.  Check that the script completes without unhandled exceptions.
    3.  Verify that an output JSON file is created.
    4.  Perform a quick, high-level check of the JSON file for presence of major sections (victim, suspects, evidence).
-   **Tools:** Shell scripts, Python script to automate execution and basic checks.
-   **Goal:** Ensure the basic pipeline is runnable and produces an output file. This is a "smoke test" to catch major breakages, not for detailed content validation (which is manual).

## 3. What Not Being Tested (for MVP)

-   **Deep Narrative Coherence/Quality via Automation:** Automated testing of the creative quality, engagement, or deep logical consistency of the LLM-generated narratives is out of scope for MVP. This relies on manual review.
-   **LLM Performance/Accuracy Directly:** We are not building a test suite to evaluate the LLMs themselves, but rather how our agent system uses them.
-   **UI/UX Testing:** No user interface beyond the CLI for the developer.
-   **Load/Stress Testing:** The system is a single-user CLI tool for MVP.
-   **Security Vulnerability Testing (beyond API key handling):** Not a focus for local CLI tool with no external network exposure beyond OpenAI API.

## 4. Test Environment & Data

-   **Environment:** Local developer machine with Python environment set up as per `requirements.txt`. OpenAI API key must be configured.
-   **Test Data:**
    -   Input themes for testing will be simple strings (e.g., "Cyberpunk", "Haunted Mansion", "Pirate Ship").
    -   For unit/integration tests requiring predictable LLM-like output, mocked responses or fixture data (small, valid JSON snippets matching Pydantic models) will be used.

## 5. Defect Tracking & Reporting

-   For this internal MVP, defects found during manual review or by automated tests will be tracked informally (e.g., a simple list, comments in code, or a basic issue tracker if the user prefers).
-   The primary goal is to identify issues for the developer/author to iterate on the agent prompts and logic.

This testing strategy aims to provide sufficient confidence in the MVP's core functionality while remaining practical for a rapid development cycle. 