# Mystery.AI - System Architecture (MVP)

## 1. Introduction

This document outlines the technical architecture for the Minimum Viable Product (MVP) of Mystery.AI, an AI-driven agentic system for generating murder mystery game narratives. The architecture is designed to fulfill the requirements specified in the `prd.md` and the defined epics (`epic1.md` through `epic4.md`).

The core of the system is a Python application leveraging the OpenAI Agents SDK to orchestrate a sequence of specialized AI agents. Each agent performs a distinct step in the mystery generation process, transforming and enriching a central data object (`CaseContext`) until a complete mystery framework is produced as a JSON output.

## 2. Architectural Goals & Constraints (MVP)

- **Primary Goal:** Create a robust and understandable local command-line tool that reliably generates the foundational elements of a murder mystery.
- **Technology Constraint:** Exclusively use Python and the OpenAI Agents SDK for all AI agent functionality and LLM interactions.
- **Modularity:** Design agents with clear, distinct responsibilities to facilitate development, testing, and future iteration.
- **Data-Driven:** Utilize Pydantic models for strong data typing and structuring throughout the agent pipeline and for the final JSON output.
- **Simplicity (MVP):** Avoid unnecessary complexity. No external databases, APIs (beyond OpenAI via SDK), or dedicated servers for the MVP.
- **Traceability:** Ensure sufficient logging and leverage SDK tracing for debugging and understanding agent behavior.

## 3. High-Level Architecture Overview

The Mystery.AI MVP employs a **sequential, multi-agent pipeline architecture**. An orchestrator (either a main script or a dedicated `OrchestratorAgent`) manages the flow, invoking a series of specialized agents in a predefined order. Each agent takes the current state of the mystery (encapsulated in a `CaseContext` Pydantic model), performs its generation task using an LLM via the OpenAI Agents SDK, and returns the updated `CaseContext` to the orchestrator.

The primary components are:
1.  **Orchestrator (`run_mystery_generation.py` / `OrchestratorAgent`):** The main entry point and controller of the generation pipeline.
2.  **Specialized Generative Agents:** A series of agents, each responsible for a specific part of the mystery creation.
3.  **Data Model (`CaseContext`):** A central Pydantic model that accumulates all generated mystery data.

## 4. Component Breakdown & Agent Responsibilities

Clear Naming Conventions

The following agents are proposed for the MVP, each interacting via the `OrchestratorAgent`:

1.  **`CaseInitializationAgent`**:
    *   **Input:** Theme (string).
    *   **Responsibility:** Generates initial victim details (Name, Occupation, Personality, Cause of Death) based on the theme.
    *   **Output:** `VictimProfile` object, added to `CaseContext`.

2.  **`SuspectGenerationAgent`**:
    *   **Input:** `CaseContext` (containing theme and `VictimProfile`).
    *   **Responsibility:** Generates 2-3 unique suspect profiles (name, description, relationship to victim).
    *   **Output:** List of `SuspectProfile` objects.

3.  **`MMOGenerationAgent`**:
    *   **Input:** `CaseContext` (theme, victim) and a single `SuspectProfile`.
    *   **Responsibility:** For the given suspect, generates a plausible Means, Motive, and Opportunity (MMO), consistent with the case.
    *   **Output:** `MMO` object for that suspect.
    *   *(Note: The Orchestrator will call this agent for each suspect generated by `SuspectGenerationAgent`.)*

4.  **`KillerSelectionAgent`**:
    *   **Input:** `CaseContext` (with list of suspects, each having their full MMO).
    *   **Responsibility:** Randomly selects one suspect as the killer and updates their `is_killer` status in the `CaseContext`.
    *   **Output:** Updated `CaseContext` with killer identified.

5.  **`MMOModificationAgent`**:
    *   **Input:** `CaseContext` (with killer identified and suspects' original MMOs).
    *   **Responsibility:** For each non-killer suspect, selects one MMO element (Means, Motive, or Opportunity) and prompts an LLM to weaken/invalidate it, creating a red herring. Stores the modified element.
    *   **Output:** Updated `CaseContext` with modified MMOs for non-killers.

6.  **`EvidenceGenerationAgent`**:
    *   **Input:** `CaseContext` (with killer, suspects' original & modified MMOs).
    *   **Responsibility:**
        *   Generates 2-3 pieces of evidence supporting the killer's true MMO.
        *   Generates 1-2 red herring evidence items for each non-killer, pointing towards their original (now weakened) MMOs.
    *   **Output:** List of `EvidenceItem` objects, added to `CaseContext`.

7.  **`JSONOutputAgent` (or functionality within Orchestrator):**
    *   **Input:** Final `CaseContext` object.
    *   **Responsibility:** Serializes the `CaseContext` to a well-formatted JSON string and writes it to a file.
    *   **Output:** JSON file.

## 5. Agent Interaction Diagram (Simplified MVP Flow)

```mermaid
graph TD
    A[CLI Input: Theme] --> B(Orchestrator);
    B -- Theme --> C(CaseInitializationAgent);
    C -- VictimProfile, Theme --> B;
    B -- CaseContext (Victim, Theme) --> D(SuspectGenerationAgent);
    D -- List[SuspectProfile] --> B;

    subgraph MMO Loop for each SuspectProfile
        direction LR
        B_Loop{Orchestrator} -- CaseContext, SuspectProfile_N --> E(MMOGenerationAgent);
        E -- MMO_N --> B_Loop;
    end

    B -- CaseContext (Victim, Theme, Suspects with Full MMOs) --> F(KillerSelectionAgent);
    F -- CaseContext (Killer Identified) --> B;
    B -- CaseContext (Killer Identified) --> G(MMOModificationAgent);
    G -- CaseContext (Non-Killer MMOs Modified) --> B;
    B -- CaseContext (Killer, Modified MMOs) --> H(EvidenceGenerationAgent);
    H -- List[EvidenceItem] --> B;
    B -- Final CaseContext --> I(JSONOutputAgent/Function);
    I --> J[Output: mystery_case.json];

    classDef agent fill:#D6EAF8,stroke:#2E86C1,stroke-width:2px;
    classDef data fill:#E8F8F5,stroke:#16A085,stroke-width:1px;
    classDef io fill:#FCF3CF,stroke:#F1C40F,stroke-width:2px;

    class A,J io;
    class C,D,E,F,G,H,I agent;
    class B,B_Loop agent;
```

## 6. Data Flow (Revised)

The primary data structure is the `CaseContext` Pydantic model. It is initialized by the `OrchestratorAgent` (or at the start of the main script) and progressively enriched as it's passed through the agent pipeline. **All Pydantic models used for agent inputs or outputs requiring structured responses from the LLM must adhere to the constraints specified by OpenAI for structured outputs (e.g., no direct `dict` types, using nested models instead; refer to OpenAI documentation on supported JSON schema features for `response_format`).**

1.  **Initial:** Contains `theme` (string).
2.  **After `CaseInitializationAgent`:** Adds `VictimProfile` (Pydantic model).
3.  **After `SuspectGenerationAgent` & `MMOGenerationAgent` loop:** Adds `List[Suspect]` (where each `Suspect` is a Pydantic model containing `SuspectProfile` and original `MMO` - both Pydantic models).
4.  **After `KillerSelectionAgent`:** `is_killer` flag (boolean) is set for each `Suspect`.
5.  **After `MMOModificationAgent`:** Modified MMO elements are added to non-killer `Suspect` objects (likely as new fields within the `Suspect` model or a nested `ModifiedMMO` Pydantic model).
6.  **After `EvidenceGenerationAgent`:** Adds `List[EvidenceItem]` (where `EvidenceItem` is a Pydantic model).
7.  This final, fully populated `CaseContext` is then serialized to JSON.

## 7. OpenAI Agents SDK Usage (Revised)

-   **Import Convention:** The SDK is installed as `openai-agents` but imported in Python code as `agents` (e.g., `from agents import Agent, Runner`). **Care must be taken to avoid naming local Python modules `agents.py` to prevent import conflicts.**
-   **Agents:** Each component (e.g., `CaseInitializationAgent`) will be defined as an `agents.Agent` instance, with specific `instructions`, `name`, and the `model` parameter specifying the LLM (e.g., `model="gpt-4.1-mini"`). Additional model parameters like temperature are passed via `model_settings=ModelSettings(...)`.
-   **Structured Output (Pydantic Models):** Agents generating structured data (like `VictimProfile`, `MMO`, `List[EvidenceItem]`) will specify an `output_type` using a Pydantic model. **These Pydantic models must strictly adhere to the data types and structures supported by the OpenAI API for `response_format` with structured outputs.** This means using primitives (string, integer, boolean, number), `Enum`, arrays (lists) of supported types, or nested objects (which translate to nested Pydantic models). Direct usage of Python `dict` types or complex `Union` types within these Pydantic models for LLM output definition is generally not supported and must be avoided by using nested Pydantic models. Refer to [OpenAI Structured Outputs Guide](https://platform.openai.com/docs/guides/structured-outputs?api-mode=responses).
-   **Runner:** The `OrchestratorAgent` (or main script) will use `agents.Runner.run()` (or `run_sync()`) to execute each specialized agent, passing the current `CaseContext` (or relevant parts) as input. The `RunConfig` object can be used with the `Runner` to customize tracing for specific runs.
-   **Handoffs (Implicit):** For this MVP, the orchestration is primarily sequential and code-driven. The `OrchestratorAgent` explicitly calls the next agent.
-   **Tracing:** The SDK's built-in tracing will be active by default. Enhanced tracing with specific workflow names, IDs, and metadata will be implemented as detailed in Section 9.

## 8. Error Handling and Resilience (MVP Focus)

For the MVP, error handling will focus on ensuring the local command-line tool can gracefully handle common issues and provide informative feedback to the developer/user.

-   **Agent-Level Errors:**
    -   Each specialized agent, when executed by the `Runner`, should include `try-except` blocks to catch potential exceptions during LLM calls (e.g., API errors, timeouts, malformed responses if not perfectly matching Pydantic `output_type`).
    -   If an LLM call fails or returns unusable data after a reasonable number of retries (TBD, could be 0-1 for MVP), the agent should log the error extensively and either:
        -   Return a specific error indicator or a partially filled data object to the orchestrator.
        -   For MVP, it might be acceptable to halt the entire process with a clear error message if a critical generation step fails.
-   **Data Validation:** Pydantic models will provide automatic validation for the `output_type` of agent responses. If an LLM returns JSON that doesn't conform to the specified Pydantic model, the Agents SDK/Pydantic will raise an error. This should be caught and logged.
-   **Orchestrator Resilience:** The main orchestrator script should handle errors returned by individual agents. For the MVP, if a critical agent fails, the orchestrator will log the failure and terminate, providing a clear message. It will not attempt complex recovery or alternative paths.
-   **Input Validation:** Basic validation of the initial CLI theme input (e.g., ensuring it's provided) will be handled by `argparse`.

## 9. Logging and Debugging (Revised)

Effective logging and tracing are crucial for understanding the behavior of the multi-agent system and for debugging.

-   **Python `logging` Module:** The standard Python `logging` module will be used for application-level logging.
    -   Configuration: Set up in the main orchestrator script (console output, log level via CLI flag).
    -   Agent Logging: Each agent logs initiation, key steps, data summaries (non-sensitive), completion, and errors.
-   **OpenAI Agents SDK Tracing:**
    -   **Utilization:** The SDK's built-in tracing capabilities will be a primary debugging tool.
    -   **Viewing Traces:** Developers will view traces via the OpenAI Dashboard (if API key configured for tracing) or other configured processors.
    -   **Enriching Traces:** For significant operations like a full mystery generation, the main orchestrator will use the `with agents.trace(...)` context manager.
        -   `workflow_name`: A descriptive name (e.g., `"MysteryGeneration_MVP"`).
        -   `trace_id`: A unique ID for the entire run (e.g., UUID generated at start).
        -   `group_id`: (Optional) Could be used if multiple related traces form part of a larger user session in future iterations.
        -   `metadata`: Key input parameters (like the theme) and relevant configuration settings will be added as metadata to the trace for better context and filterability.
    -   **Sensitive Data:** Tracing of sensitive data in LLM inputs/outputs or function calls can be controlled via `RunConfig.trace_include_sensitive_data` if necessary. Refer to [OpenAI Agents SDK Tracing Documentation](https://openai.github.io/openai-agents-python/tracing/).
-   **Structured Output:** The final JSON output itself serves as a key debugging artifact, allowing inspection of the full generated mystery.

## 10. Configuration Management

For the MVP, configuration will be kept simple:

-   **OpenAI API Key:** Managed via an environment variable (`OPENAI_API_KEY`) loaded using `python-dotenv` from a `.env` file.
-   **CLI Arguments:**
    -   The primary input (e.g., `--theme`) will be passed via CLI arguments using `argparse` in the main script (`run_mystery_generation.py`).
    -   A `--debug` flag could control log verbosity.
-   **Agent-Specific Parameters (Initial Approach):**
    -   Parameters like the specific LLM model to use for an agent (e.g., `gpt-4.1-mini` for one, `o4-mini` for another), number of suspects to generate (2-3), or number of evidence items, will initially be hardcoded as constants within the agent definitions or the orchestrator script for simplicity in the MVP.
    -   **Future Consideration:** These could be moved to a simple configuration file (e.g., `config.json` or `config.py`) if more flexibility is needed during development or for experimentation beyond the immediate MVP scope.

## 11. Security Considerations (MVP Focus)

Given the local execution nature and MVP scope, security concerns are minimal but good practices will be followed:

-   **API Key Security:** The `OPENAI_API_KEY` is the primary secret. It will be stored in a `.env` file, which will be added to `.gitignore` to prevent accidental commits. A `.env.example` file will be provided.
-   **Input Sanitization:** The CLI theme input is a simple string. No complex parsing or direct OS command execution based on this input is planned.
-   **Output Data:** The generated JSON data contains fictional mystery elements. No real PII or sensitive production data is handled.
-   **Dependencies:** Keep Python package dependencies to a minimum and use well-known libraries to reduce supply chain risks. Regular updates are not a focus for the MVP's short lifecycle but would be for a longer-lived project.

## 12. Future Architectural Considerations (Post-MVP)

While the MVP focuses on a streamlined local generation process, future enhancements outlined in the PRD would necessitate architectural evolution:

-   **Database Integration:** For persistent storage of generated mysteries, user accounts, or more complex knowledge bases, a database (e.g., PostgreSQL, MongoDB) would be required.
-   **Web Interface/API:**
    -   To support user-facing interactions (prompt input, game play) or a tool for the primary author, a web framework (e.g., FastAPI, Django, Flask for an API; Streamlit, Gradio, or React for a UI) would be needed.
    -   This implies considerations for authentication, authorization, session management, and web security.
-   **Advanced Agent Orchestration:**
    -   Moving beyond a purely sequential pipeline to more dynamic, LLM-driven handoffs or parallel agent execution using the advanced features of the Agents SDK.
    -   Implementing more sophisticated "meta-agents" or "evaluator agents" for coherence checking (Red Team Agent, Ripple Effect Analyzer, etc.) would require more complex interaction patterns.
-   **Task Queues & Asynchronous Processing:** For longer generation tasks or a web-based service, using task queues (e.g., Celery with Redis/RabbitMQ) might become necessary to handle requests asynchronously.
-   **Scalability & Cloud Deployment:** If the tool gains traction or is offered as a service, deploying to a cloud platform (AWS, GCP, Azure) with considerations for auto-scaling, load balancing, and managed services would be essential.
-   **Enhanced Configuration Management:** A more robust configuration system (e.g., using dedicated config libraries, environment-specific configs) would be needed.
-   **CI/CD Pipeline:** For automated testing, building, and deployment.

## 13. Player View Output Generation (MVP Enhancement)

In addition to the main structured JSON output, the system will generate a human-readable "Player View" file in Markdown format. This output is intended for easier review and playtesting of the generated mystery, primarily for the developer/author during the MVP phase and beyond.

-   **Purpose:** To present the initial mystery setup (theme, victim) and the discoverable elements (suspect profiles, evidence descriptions) in a shuffled, narrative-friendly format, withholding solution-critical information (killer identity, full MMOs, evidence connections/types).
-   **Module:** Logic for this will reside in `src/mystery_ai/core/player_view_generator.py`.
-   **Process:**
    1.  After the complete `CaseContext` is generated and the main JSON file is written, the `main_orchestrator.py` will call a function in `player_view_generator.py`.
    2.  This function will extract specific fields from the `CaseContext` into a dedicated `PlayerViewData` Pydantic model (subset of `CaseContext`).
    3.  The lists of suspects (profiles including name, description, relationship to victim) and evidence items (descriptions only) within `PlayerViewData` will be randomly shuffled.
    4.  The `PlayerViewData` will be formatted into a Markdown string with appropriate headings and lists.
    5.  This string will be saved to a file (e.g., `mystery_<theme>_<timestamp>_player_view.md`) in the `generated_mysteries/` directory.
-   **Data Model:** A `PlayerViewData` Pydantic model will be defined in `src/mystery_ai/core/data_models.py` to represent the data subset for this view. 