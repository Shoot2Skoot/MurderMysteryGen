# Story 7.3: Enhance `EvidenceGenerationAgent` for Richer Evidence

**Status:** COMPLETED

## Goal & Context

**User Story:** As a Developer, I want to modify the `EvidenceGenerationAgent` to select an evidence category, generate a narrative function description, and populate these new fields in the `EvidenceItem`.

**Context:** This story builds directly on Story 7.1 (Define Master List of Evidence Categories) and Story 7.2 (Update `EvidenceItem` Data Model). The `EvidenceGenerationAgent` will now be enhanced to:
1.  Accept a list of allowed evidence categories (provided by the orchestrator, Story 7.4).
2.  Select a category from this list for each piece of evidence it generates.
3.  Generate a textual description of the evidence's narrative function (e.g., its subtlety, directness, role as a red herring).
4.  Populate the new `evidence_category` and `narrative_function_description` fields in the `EvidenceItem` objects it outputs.

## Detailed Requirements

- Modify `src/mystery_ai/agents/evidence_generator.py`.
- Update the `EvidenceGenerationAgent`'s input handling to accept a new parameter: a list of allowed `Evidence Categories` (strings).
- Modify the agent's internal logic/prompt:
    - For each piece of evidence generated for a suspect, the agent must select an appropriate `evidence_category` from the provided list of options.
    - The agent must also generate a `narrative_function_description` (a string). This description should explain the intended narrative role of the evidence â€“ e.g., "This clue directly implicates the suspect if their prior argument is known to be false," "A subtle clue that hints at the suspect's presence, but requires connecting with another piece of information," "This is a red herring designed to mislead by suggesting an alternative motive."
    - The agent must populate the `evidence_category` and `narrative_function_description` fields (added in Story 7.2) in each `EvidenceItem` object it creates.
- The agent should still generate the evidence `description` as before, ensuring it's consistent with the chosen category and narrative function.

## Acceptance Criteria (ACs)

- AC1: `EvidenceGenerationAgent` correctly processes a new input parameter: a list of allowed evidence category options.
- AC2: For each `EvidenceItem` generated, the agent successfully selects an `evidence_category` from the provided list of options and populates the `EvidenceItem.evidence_category` field.
- AC3: For each `EvidenceItem` generated, the agent generates a meaningful textual description for its narrative function and populates the `EvidenceItem.narrative_function_description` field.
- AC4: Both `evidence_category` and `narrative_function_description` fields are correctly populated in all `EvidenceItem` objects returned by the agent for each suspect.
- AC5: The chosen `evidence_category`, the `narrative_function_description`, and the main evidence `description` are coherent with each other and with the evidence's intended role (e.g., true clue for the killer, red herring for a non-killer).

## Technical Implementation Context

**Guidance:** Use the following details for implementation. Developer agent is expected to follow project standards in `docs/coding-standards.md` and understand the project structure in `docs/project-structure.md`.

- **Relevant Files:**
  - Files to Create: None.
  - Files to Modify:
    - `src/mystery_ai/agents/evidence_generator.py` (agent instructions/prompt).

- **Key Technologies:**
  - Python.
  - LLM Prompt Engineering.

- **API Interactions / SDK Usage:**
  - Modifying existing agent instructions.

- **UI/UX Notes:**
  - N/A

- **Data Structures:**
  - Input: In addition to existing context (suspect profile, MMO, theme, etc.), the agent will receive `evidence_category_options: List[str]`.
  - Output: A list of `EvidenceItem` Pydantic model instances, now with the new fields populated.

- **Environment Variables:**
  - None specific to this story.

- **Coding Standards Notes:**
  - Follow standards in `docs/coding-standards.md`.
  - Prompt engineering will be critical to guide the LLM in selecting an appropriate category and generating a useful, concise narrative function description that aligns with the evidence itself and its role. Examples in the prompt may be very helpful.

## Testing Requirements

**Guidance:** Verify implementation against the ACs using the following tests.

- **Integration Tests (Agent Level):**
  - Test `EvidenceGenerationAgent` by providing mocked inputs, including a list of evidence category options.
  - Verify that each generated `EvidenceItem` has `evidence_category` populated from the provided options and a `narrative_function_description` populated.
  - Manually review the coherence of these new fields with the evidence description and its intended role (AC5).
- **Manual/CLI Verification:**
  - Run the main orchestration.
  - Inspect the `evidence_items` in the final JSON output.
  - Check `evidence_category` and `narrative_function_description` for correctness, variety, and coherence.
  - Add logging in the agent to show the category options received, the category chosen, and the narrative function generated for each piece of evidence.

## Tasks / Subtasks

- [ ] Task 1: Update the `EvidenceGenerationAgent`'s instructions/prompt in `src/mystery_ai/agents/evidence_generator.py` to describe the new input parameter: `evidence_category_options: List[str]`.
- [ ] Task 2: Modify the prompt to instruct the LLM to select one category from the `evidence_category_options` for each piece of evidence it generates.
- [ ] Task 3: Modify the prompt to instruct the LLM to generate a concise `narrative_function_description` for each piece of evidence, explaining its purpose or subtlety.
- [ ] Task 4: Ensure the LLM populates the `EvidenceItem.evidence_category` and `EvidenceItem.narrative_function_description` fields.
- [ ] Task 5: Ensure the main evidence `description` remains coherent with the new fields.
- [ ] Task 6: Add logging within the agent for received category options, chosen category, and generated narrative function.
- [ ] Task 7: Test with various suspect profiles and category options to ensure functionality and coherence (AC5).
- [ ] Task 8: Verify all Acceptance Criteria are met.

## Story Wrap Up (Agent Populates After Execution)

- **Agent Model Used:** `<Agent Model Name/Version>`
- **Completion Notes:** `EVIDENCE_GENERATOR_AGENT_INSTRUCTIONS` updated to accept `evidence_category_options`, select a category, generate `narrative_function_description`, and populate these in `EvidenceItem`. Prompt refined multiple times to emphasize creative, non-literal evidence descriptions. `prepare_evidence_generation_input` updated to accept options (though initially hardcoded, fixed in 7.4).
- **Change Log:**
  - Initial Draft
  - Marked as COMPLETED. 